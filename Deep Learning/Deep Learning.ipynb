{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd28f13b",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch Cheat Sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c58dd41",
   "metadata": {},
   "source": [
    "###  PyTorch is an open-source machine learning library primarily developed by Facebook's AI Research Lab (FAIR). It is widely used for various machine learning and deep learning tasks, including neural network development, natural language processing (NLP), computer vision, and reinforcement learning. In this cheat sheet, learn all the fundamentals of working with PyTorch in one convenient location!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d58f9fe",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "1. PyTorch is one of the most popular deep learning frameworks, with a syntax similar to NumPy.\n",
    "2. In the context of PyTorch, you can think of a Tensor as a NumPy array that can be run on a CPU or a GPU, and has a method for automatic differentiation (needed for backpropagation).\n",
    "3. TorchText, TorchVision, and TorchAudio are Python packages that provide PyTorch with functionality for text, image, and audio data respectively\n",
    "4. A neural network consists of neurons that are arranged into layers. Input values are passed to the first layer of neural networks. Each neuron has two properties: a weight and a bias. The output of a neuron in a neural network is a weighted sum of its inputs, plus the bias. The output is passed on to any connected neurons in the next layer, and this continues until the final layer of the network is reached.\n",
    "5. An activation function is a transformation of the output from a neuron, and is used to introduce non-linearity into the calculations.\n",
    "6. Backpropagation is an algorithm used to train neural networks by iteratively adjusting the weights and biases of each neuron.\n",
    "7. Saturation is when the output from a neuron reaches a maximum or minimum value beyond which it cannot change. This can reduce learning performance, and an activation function such as ReLU may be needed to avoid the phenomenon.\n",
    "8. The loss function quantifies the difference between the predicted output of a model and the actual target output\n",
    "9. The optimizer is an algorithm to adjust the parameters (neuron weights and biases) of a neural network during the training process in order to minimize the loss function.\n",
    "10. The learning rate controls the step size of the optimizer. If the learning rate is too low the optimization will take too long. If it is too high, the optimizer will not effectively minimize the loss function leading to poor predictions.\n",
    "11. Momentum controls the inertia of the optimizer. If momentum is too low, the optimizer can get stuck at a local minimum and give the wrong answer. If it is too high, the optimizer can fail to converge and not give an answer.\n",
    "12. Transfer learning is reusing a model trained on one task for a second similar task to accelerate the training process.\n",
    "13. Fine-tuning is a type of transfer learning where early layers are frozen, and only the layers close to the output are trained.\n",
    "14. Accuracy is a metric to determine how well a model fits a dataset. It quantifies the proportion of correctly predicted outcomes (either classifications or predictions) compared to the total number of data points in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5141893a",
   "metadata": {},
   "source": [
    "# Importing PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043a4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the top-level package for core functionality\n",
    "import torch\n",
    "\n",
    "# Import neural network functionality\n",
    "from torch import nn\n",
    "\n",
    "# Import functional programming tools\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import optimization functionality\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import dataset functions\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Import evaluation metrics\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e32298",
   "metadata": {},
   "source": [
    "# Working with Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51447724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor from list with tensor()\n",
    "tnsr = torch.tensor([1, 3, 6, 10])\n",
    "\n",
    "# Get data type of tensor elements with .dtype\n",
    "tnsr.dtype # Returns torch.int64\n",
    "\n",
    "# Get dimensions of tensor with .Size()\n",
    "tnsr.shape # Returns torch.Size([4])\n",
    "\n",
    "# Get memory location of tensor with .device\n",
    "tnsr.device # Returns cpu or gpu\n",
    "\n",
    "# Create a tensor of zeros with zeros()\n",
    "tnsr_zrs = torch.zeros(2, 3)\n",
    "\n",
    "# Create a random tensor with rand()\n",
    "tnsr_rndm = torch.rand(size=(3, 4)) # Tensor has 3 rows, 4 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022e76b",
   "metadata": {},
   "source": [
    "# Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8313d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from a pandas DataFrame with TensorDataset()\n",
    "X = df[feature_columns].values\n",
    "y = df[target_column].values\n",
    "dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float())\n",
    "\n",
    "# Load the data in batches with DataLoader()\n",
    "dataloader = DataLoader(dataset, batch_size=n, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bebc7bb",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e6b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables with one_hot()\n",
    "F.one_hot(torch.tensor([0, 1, 2]), num_classes=3) # Returns tensor of 0s and 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaeee6e",
   "metadata": {},
   "source": [
    "# Sequential Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e22f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear layer with m inputs, n outputs with Linear()\n",
    "lnr = nn.Linear(m, n)\n",
    "\n",
    "# Get weight of layer with .weight\n",
    "lnr.weight\n",
    "\n",
    "# Get bias of layer with .bias\n",
    "lnr.bias\n",
    "\n",
    "# Create a sigmoid activation layer for binary classification with Sigmoid()\n",
    "nn.Sigmoid()\n",
    "\n",
    "# Create a softmax activation layer for multi-class classification with Softmax()\n",
    "nn.Softmax(dim=-1)\n",
    "\n",
    "# Create a rectified linear unit activation layer to avoid saturation with ReLU()\n",
    "nn.ReLU()\n",
    "\n",
    "# Create a leaky rectified linear unit activation layer to avoid saturation with LeakyReLU()\n",
    "nn.LeakyReLU(negative_slope=0.05)\n",
    "\n",
    "# Create a dropout layer to regularize and prevent overfitting with Dropout()\n",
    "nn.Dropout(p=0.5)\n",
    "\n",
    "# Create a sequential model from layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_features, i),\n",
    "    nn.Linear(i, j),   # Input size must match output from previous layer\n",
    "    nn.Linear(j, n_classes),\n",
    "    nn.Softmax(dim=-1) # Activation layer comes last\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb623a97",
   "metadata": {},
   "source": [
    "# Fitting a model and calculating loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102c7be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a model to input data with model where model is a variable created by, e.g., Sequential()\n",
    "prediction = model(input_data).double()\n",
    "\n",
    "# Get target values\n",
    "actual = torch.tensor(target_values).double()\n",
    "\n",
    "# Calculate the mean-squared error loss for regression with MSELoss()\n",
    "mse_loss = nn.MSELoss()(prediction, actual) # Returns tensor(x) \n",
    "\n",
    "# Calculate the L1 loss for robust regression with SmoothL1Loss()\n",
    "l1_loss = nn.SmoothL1Loss()(prediction, actual) # Returns tensor(x) \n",
    "\n",
    "# Calculate binary cross-entropy loss for binary classification with BCELoss()\n",
    "bce_loss = nn.BCELoss()(prediction, actual) # Returns tensor(x) \n",
    "\n",
    "# Calculate cross-entropy loss for multi-class classification with CrossEntropyLoss()\n",
    "ce_loss = nn.CrossEntropyLoss()(prediction, actual) # Returns tensor(x) \n",
    "\n",
    "# Calculate the gradients via backprogagation with .backward()\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6314b17",
   "metadata": {},
   "source": [
    "# Working with Optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce34b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stochastic gradient descent optimizer with SGD(), setting learning rate and momentum\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n",
    "\n",
    "# Update neuron parameters with .step()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b0e8a3",
   "metadata": {},
   "source": [
    "# The Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45311d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to training mode\n",
    "model.train()\n",
    "# Set a loss criterion and an optimizer\n",
    "loss_criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n",
    "# Loop over chunks of data in the training set\n",
    "for data in dataloader:\n",
    "    # Set the gradients to zero with .zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "    # Get features and targets for current chunk of data\n",
    "    features, targets = data\n",
    "    # Run a \"forward pass\" to fit the model to the data\n",
    "    predictions = model(data)\n",
    "    # Calculate loss\n",
    "    loss = loss_criterion(predictions, targets)\n",
    "    # Calculate gradients using backprogagation\n",
    "    loss.backward()\n",
    "    # Update the model parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1ad835",
   "metadata": {},
   "source": [
    "# The Evaluation Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b07e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create accuracy metric with Accuracy()\n",
    "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n",
    "# Loop of chunks of data in the validation set\n",
    "for i, data in enumerate(dataloader, 0):\n",
    "    # Get features and targets for current chunk of data\n",
    "    features, targets = data\n",
    "    # Run a \"forward pass\" to fit the model to the data\n",
    "    predictions = model(data)\n",
    "    # Calculate accuracy over the batch\n",
    "    accuracy = metric(output, predictions.argmax(dim=-1))\n",
    "# Calculate accuracy over all the validation data\n",
    "accuracy = metric.compute()\n",
    "print(f\"Accuracy on all data: {accuracy}\")\n",
    "# Reset the metric for the next dataset (training or validation)\n",
    "metric.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00fb77d",
   "metadata": {},
   "source": [
    "# Transfer Learning and Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536dead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a layer of a model to a file with save()\n",
    "torch.save(layer, 'layer.pth')\n",
    "\n",
    "# Load a layer of a model from a file with load()\n",
    "new_layer = torch.load('layer.pth')\n",
    "\n",
    "# Freeze the weight for layer 0 with .requires_grad\n",
    "for name, param in model.named_parameters():\n",
    "    if name == \"0.weight\":\n",
    "        param.requires_grad = False\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
